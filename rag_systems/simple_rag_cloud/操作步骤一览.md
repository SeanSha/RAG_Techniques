# 云端 GPU 最小验证：操作步骤一览

按下面顺序做，做完一步再下一步。**推荐用 RunPod 的 Web Terminal**，这样不用在本机配 SSH。

---

## 第一步：开一台 GPU 机子（RunPod）

1. 打开 **https://www.runpod.io** → 注册/登录。
2. **Billing** → 充值（例如 10 美元）。
3. **Pods** → **+ Deploy**。
4. 选择：
   - **GPU**：RTX 3060 或 T4
   - **Container Image**：RunPod Pytorch 2.1 或 Ubuntu 22.04
   - **Disk**：50 GB
5. 点 **Deploy**，等状态变成 **Running**。
6. 点进该 Pod → **Connect** → **Start Web Terminal**（浏览器里会打开一个终端，这就是「云实例终端」）。

---

## 第二步：在云实例里装 Ollama 和模型

在 **Web Terminal** 里整段复制、粘贴执行：

```bash
curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve > /tmp/ollama.log 2>&1 &
sleep 10
ollama pull llama3.2:3b
ollama list
```

看到列表里有 `llama3.2:3b` 就对了。

---

## 第三步：在云实例里装 Python 和依赖

继续在 **同一 Web Terminal** 里执行：

```bash
# 装 Miniconda
wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh
bash /tmp/miniconda.sh -b -p $HOME/miniconda3
export PATH="$HOME/miniconda3/bin:$PATH"

# 创建环境
conda create -n rag python=3.11 -y
source $(conda info --base)/etc/profile.d/conda.sh
conda activate rag
```

先不要关终端，接着做第四步把代码弄上去，再回来装依赖。

---

## 第四步：把 simple_rag_local 代码和 PDF 弄到云实例上

任选一种方式。

### 方式 A：项目在 GitHub 上（最简单）

在 **Web Terminal** 里执行（把 `<你的用户名>` 和 `<仓库名>` 换成你自己的）：

```bash
cd ~
git clone https://github.com/<你的用户名>/<仓库名>.git RAG_Techniques
```

然后把 PDF 传到 `data/`：若本机有 WSL，在本机**另开一个终端**执行（把 `端口`、`实例IP`、`私钥路径` 换成 RunPod 页面上给你的 SSH 信息）：

```bash
scp -P 端口 -i 私钥路径 /mnt/d/Coding/RAGExamples/RAG_Techniques/rag_systems/simple_rag_local/data/Understanding_Climate_Change.pdf root@实例IP:~/RAG_Techniques/rag_systems/simple_rag_local/data/
```

若暂时没有 PDF，可以后面用 `--path /路径/随便一个.pdf` 做「能跑通」的验证。

### 方式 B：本机用 SCP 上传整个项目（不用 GitHub）

在本机 **WSL 或 Git Bash** 里（把 `端口`、`实例IP`、`私钥路径` 换成 RunPod 给的）：

```bash
cd /mnt/d/Coding/RAGExamples/RAG_Techniques
scp -P 端口 -r -i 私钥路径 rag_systems root@实例IP:~/RAG_Techniques/
```

这样 `simple_rag_local` 和里面的 `data/`、PDF 都会上去。

---

## 第五步：在云实例上装项目依赖并跑一次

回到 **Web Terminal**，确认代码已经在 `~/RAG_Techniques/rag_systems/simple_rag_local`（若用方式 A 或 B 上传，路径就是这个）。执行：

```bash
cd ~/RAG_Techniques/rag_systems/simple_rag_local
pip install -r requirements.txt
python main.py --query "what is the main cause of climate change?"
```

若看到：**Chunking Time** → **Retrieval Time** → **Context 1 / Context 2** → 一段英文回答，说明**云端 GPU + Ollama + simple_rag_local 的最小验证已经跑通**。

（若报错找不到 PDF，检查 `data/` 下是否有 `Understanding_Climate_Change.pdf`，或用 `--path /路径/你的.pdf` 指定。）

---

## 第六步：用完关机

RunPod 里对该 Pod 点 **Stop** 或 **Terminate**，停止计费。下次要用再 Deploy 或开机即可。

---

## 常见问题

| 情况 | 处理 |
|------|------|
| Web Terminal 没有 SSH 密钥，没法用 SCP | 用方式 A：在实例上 `git clone` 你的仓库，PDF 稍后用 RunPod 的 **Connect → SFTP** 或文件浏览器上传（若有）。 |
| `ollama list` 没有模型 | 再执行一次 `ollama pull llama3.2:3b`，等下载完成。 |
| `python main.py` 报错连不上 Ollama | 确认前面执行过 `nohup ollama serve ...`，再执行 `ollama list` 能列出模型说明服务在跑。 |
| 没有 PDF | 用 `--path /root/某个小.pdf` 指定任意一个小 PDF 先验证流程；或从本机 SCP 上传 PDF 到 `~/RAG_Techniques/rag_systems/simple_rag_local/data/`。 |

更详细的说明和可选方案见 **[CLOUD_GPU_DEPLOY.md](./CLOUD_GPU_DEPLOY.md)**。
